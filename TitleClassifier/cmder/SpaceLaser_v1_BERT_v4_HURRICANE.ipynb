{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528564cb",
   "metadata": {},
   "source": [
    "# BLS SOC Manual - Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "266a2294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text contains 188 tokens.\n",
      "They are: ['computer', 'and', 'mathematical', 'occupations', '-', '>', 'not', 'found', '-', '>', 'software', 'and', 'web', 'developers', ',', 'programmers', ',', 'and', 'test', '##ers', '-', '>', 'web', 'and', 'digital', 'interface', 'designers', '-', '>', 'graphic', 'web', 'designer', 'design', 'digital', 'user', 'interfaces', 'or', 'websites', '.', 'develop', 'and', 'test', 'layout', '##s', ',', 'interfaces', ',', 'functionality', ',', 'and', 'navigation', 'menu', '##s', 'to', 'ensure', 'compatibility', 'and', 'usa', '##bility', 'across', 'browser', '##s', 'or', 'devices', '.', 'may', 'use', 'web', 'framework', 'applications', 'as', 'well', 'as', 'client', '-', 'side', 'code', 'and', 'processes', '.', 'may', 'evaluate', 'web', 'design', 'following', 'web', 'and', 'accessibility', 'standards', ',', 'and', 'may', 'analyze', 'web', 'use', 'metric', '##s', 'and', 'opt', '##imi', '##ze', 'websites', 'for', 'market', '##ability', 'and', 'search', 'engine', 'ranking', '.', 'may', 'design', 'and', 'test', 'interfaces', 'that', 'facilitate', 'the', 'human', '-', 'computer', 'interaction', 'and', 'maximize', 'the', 'usa', '##bility', 'of', 'digital', 'devices', ',', 'websites', ',', 'and', 'software', 'with', 'a', 'focus', 'on', 'aesthetics', 'and', 'design', '.', 'may', 'create', 'graphics', 'used', 'in', 'websites', 'and', 'manage', 'website', 'content', 'and', 'links', '.', 'exclude', '##s', '“', 'special', 'effects', 'artists', 'and', 'animator', '##s', '”', '(', '27', '-', '101', '##4', ')', 'and', '“', 'graphic', 'designers', '”', '(', '27', '-', '102', '##4', ')', '.', '15', '-', '125', '##5']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Your text\n",
    "text = \"\"\"\n",
    "Computer and Mathematical Occupations -> not found -> Software and Web Developers, Programmers, and Testers -> Web and Digital Interface Designers -> Graphic Web Designer\tDesign digital user interfaces or websites. Develop and test layouts, interfaces, functionality, and navigation menus to ensure compatibility and usability across browsers or devices. May use web framework applications as well as client-side code and processes. May evaluate web design following web and accessibility standards, and may analyze web use metrics and optimize websites for marketability and search engine ranking. May design and test interfaces that facilitate the human-computer interaction and maximize the usability of digital devices, websites, and software with a focus on aesthetics and design. May create graphics used in websites and manage website content and links. Excludes “Special Effects Artists and Animators” (27-1014) and “Graphic Designers” (27-1024).\t15-1255\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Print the number of tokens\n",
    "print(f'The text contains {len(tokens)} tokens.')\n",
    "\n",
    "# If you want to see the tokens, you can print them too:\n",
    "print(f'They are: {tokens}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6fc80",
   "metadata": {},
   "source": [
    "# Space Laser - Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef90733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [JobDesc_BlobText]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [JobDesc_BlobText]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6520\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv('C://Offline_Storage//radiantClass//JobDesc_BlobText.csv')\n",
    "\n",
    "# Check for non-string rows\n",
    "non_string_rows = df[df['JobDesc_BlobText'].apply(lambda x: not isinstance(x, str))]\n",
    "print(non_string_rows)\n",
    "\n",
    "# Check for NaN in 'JobDesc_BlobText' column\n",
    "print(df[df['JobDesc_BlobText'].isna()])\n",
    "\n",
    "# Drop NaN rows\n",
    "df = df.dropna(subset=['JobDesc_BlobText'])\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_pt=True)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize data\n",
    "inputs = tokenizer(df['JobDesc_BlobText'].tolist(), return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Create data collator. This will automatically mask tokens 15% of the time.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "print(f\"Train size: {len(inputs['input_ids'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28b5617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "batch_size = 32  # Adjust as per your GPU memory\n",
    "\n",
    "# Create a TensorDataset from the tokenized data\n",
    "data = TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(data, sampler=RandomSampler(data), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd76038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Define the optimizer. AdamW is the Adam optimizer with weight decay fix.\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)  # You can adjust the learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb672ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print current activity and time\n",
    "from datetime import datetime\n",
    "\n",
    "def print_activity(activity):\n",
    "    current_time = datetime.now().strftime('%I:%M %p') # This will give you time in the format '01:37 PM'\n",
    "    print(f\"Starting {activity} at {current_time}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238afa45",
   "metadata": {},
   "source": [
    "# Verify CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c83ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Cuda available?: True! :-)\n",
      "CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is Cuda available?: {torch.cuda.is_available()}! :-)\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc9875",
   "metadata": {},
   "source": [
    "# Pre-Training Loop (~X min on first pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c45070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Pre-training Epoch #0 at 04:08 PM <3\n",
      "Batch 5 loaded at 04:09 PM! :-)\n",
      "Batch 10 loaded at 04:09 PM! :-)\n",
      "Batch 15 loaded at 04:10 PM! :-)\n",
      "Batch 20 loaded at 04:10 PM! :-)\n",
      "Batch 25 loaded at 04:11 PM! :-)\n",
      "Batch 30 loaded at 04:11 PM! :-)\n",
      "Batch 35 loaded at 04:12 PM! :-)\n",
      "Batch 40 loaded at 04:12 PM! :-)\n",
      "Batch 45 loaded at 04:13 PM! :-)\n",
      "Batch 50 loaded at 04:13 PM! :-)\n",
      "Batch 55 loaded at 04:14 PM! :-)\n",
      "Batch 60 loaded at 04:14 PM! :-)\n",
      "Batch 65 loaded at 04:15 PM! :-)\n",
      "Batch 70 loaded at 04:15 PM! :-)\n",
      "Batch 75 loaded at 04:16 PM! :-)\n",
      "Batch 80 loaded at 04:16 PM! :-)\n",
      "Batch 85 loaded at 04:17 PM! :-)\n",
      "Batch 90 loaded at 04:17 PM! :-)\n",
      "Batch 95 loaded at 04:18 PM! :-)\n",
      "Batch 100 loaded at 04:18 PM! :-)\n",
      "Batch 105 loaded at 04:19 PM! :-)\n",
      "Batch 110 loaded at 04:19 PM! :-)\n",
      "Batch 115 loaded at 04:20 PM! :-)\n",
      "Batch 120 loaded at 04:20 PM! :-)\n",
      "Batch 125 loaded at 04:21 PM! :-)\n",
      "Batch 130 loaded at 04:21 PM! :-)\n",
      "Batch 135 loaded at 04:22 PM! :-)\n",
      "Batch 140 loaded at 04:22 PM! :-)\n",
      "Batch 145 loaded at 04:23 PM! :-)\n",
      "Batch 150 loaded at 04:23 PM! :-)\n",
      "Batch 155 loaded at 04:24 PM! :-)\n",
      "Batch 160 loaded at 04:24 PM! :-)\n",
      "Batch 165 loaded at 04:25 PM! :-)\n",
      "Batch 170 loaded at 04:25 PM! :-)\n",
      "Batch 175 loaded at 04:26 PM! :-)\n",
      "Batch 180 loaded at 04:26 PM! :-)\n",
      "Batch 185 loaded at 04:27 PM! :-)\n",
      "Batch 190 loaded at 04:27 PM! :-)\n",
      "Batch 195 loaded at 04:28 PM! :-)\n",
      "Batch 200 loaded at 04:28 PM! :-)\n",
      "Pre-training Epoch Complete\n",
      "Epoch 1/2, Training Loss: 0.2171\n",
      "Starting Pre-training Epoch #1 at 04:29 PM <3\n",
      "Batch 205 loaded at 04:29 PM! :-)\n",
      "Batch 210 loaded at 04:29 PM! :-)\n",
      "Batch 215 loaded at 04:30 PM! :-)\n",
      "Batch 220 loaded at 04:30 PM! :-)\n",
      "Batch 225 loaded at 04:31 PM! :-)\n",
      "Batch 230 loaded at 04:31 PM! :-)\n",
      "Batch 235 loaded at 04:32 PM! :-)\n",
      "Batch 240 loaded at 04:32 PM! :-)\n",
      "Batch 245 loaded at 04:33 PM! :-)\n",
      "Batch 250 loaded at 04:33 PM! :-)\n",
      "Batch 255 loaded at 04:34 PM! :-)\n",
      "Batch 260 loaded at 04:34 PM! :-)\n",
      "Batch 265 loaded at 04:35 PM! :-)\n",
      "Batch 270 loaded at 04:35 PM! :-)\n",
      "Batch 275 loaded at 04:36 PM! :-)\n",
      "Batch 280 loaded at 04:36 PM! :-)\n",
      "Batch 285 loaded at 04:37 PM! :-)\n",
      "Batch 290 loaded at 04:37 PM! :-)\n",
      "Batch 295 loaded at 04:38 PM! :-)\n",
      "Batch 300 loaded at 04:38 PM! :-)\n",
      "Batch 305 loaded at 04:39 PM! :-)\n",
      "Batch 310 loaded at 04:39 PM! :-)\n",
      "Batch 315 loaded at 04:40 PM! :-)\n",
      "Batch 320 loaded at 04:40 PM! :-)\n",
      "Batch 325 loaded at 04:41 PM! :-)\n",
      "Batch 330 loaded at 04:41 PM! :-)\n",
      "Batch 335 loaded at 04:42 PM! :-)\n",
      "Batch 340 loaded at 04:42 PM! :-)\n",
      "Batch 345 loaded at 04:43 PM! :-)\n",
      "Batch 350 loaded at 04:43 PM! :-)\n",
      "Batch 355 loaded at 04:44 PM! :-)\n",
      "Batch 360 loaded at 04:44 PM! :-)\n",
      "Batch 365 loaded at 04:45 PM! :-)\n",
      "Batch 370 loaded at 04:45 PM! :-)\n",
      "Batch 375 loaded at 04:46 PM! :-)\n",
      "Batch 380 loaded at 04:46 PM! :-)\n",
      "Batch 385 loaded at 04:47 PM! :-)\n",
      "Batch 390 loaded at 04:47 PM! :-)\n",
      "Batch 395 loaded at 04:48 PM! :-)\n",
      "Batch 400 loaded at 04:48 PM! :-)\n",
      "Batch 405 loaded at 04:49 PM! :-)\n",
      "Pre-training Epoch Complete\n",
      "Epoch 2/2, Training Loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 2  # Number of training epochs. You can adjust this.\n",
    "batchcount = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    current_loopTime = datetime.now().strftime('%I:%M %p')\n",
    "    print(f\"Starting Pre-training Epoch #{epoch} at {current_loopTime} <3\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(dataloader):\n",
    "\n",
    "        # Load batch to device\n",
    "        batchcount += 1\n",
    "        b_input_ids, b_input_mask = batch[0].to(device), batch[1].to(device)\n",
    "        current_batchTime = datetime.now().strftime('%I:%M %p')\n",
    "        if batchcount % 5 == 0:\n",
    "            print(f\"Batch {batchcount} loaded at {current_batchTime}! :-)\")\n",
    "        \n",
    "        # Clear out the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_input_ids) # labels for MLM is the same input_ids\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Pre-training Epoch Complete\")\n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Start validation loop\n",
    "    #model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    #val_total_loss = 0\n",
    "    \n",
    "    #for batch in val_dataloader:\n",
    "        #b_input_ids, b_input_mask = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        #with torch.no_grad():  # Don't compute gradients for validation\n",
    "            #outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_input_ids)\n",
    "            #val_loss = outputs.loss\n",
    "            #val_total_loss += val_loss.item()\n",
    "\n",
    "    #val_avg_loss = val_total_loss / len(val_dataloader)\n",
    "    #print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fde771",
   "metadata": {},
   "source": [
    "# Save the Model :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b6b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"SLv1_Bv4_model_entire_9-11-23_945am.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c2e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
